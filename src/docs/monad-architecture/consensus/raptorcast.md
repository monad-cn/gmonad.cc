# RaptorCast

### 摘要

RaptorCast 是 MonadBFT 中采用的专用组播消息传输协议，用于将领导者提出的区块提案发送给所有验证者。该协议基于 [RFC 5053 标准](https://datatracker.ietf.org/doc/html/rfc5053)中的 Raptor 码将区块提案转换为纠删码数据块，并通过双层广播树结构进行传输：第一级由单个非领导者节点承担，每个非领导者节点负责为不同的数据块子集担任一级中转节点，其分配的数据块比例与该验证者的权益权重成正比。

通过这种设计，RaptorCast 既能充分利用全网节点的上行带宽快速将区块提案传播至所有验证者，又完整保留了拜占庭容错能力。


> 敬请查阅 Category Labs 发布的这篇[博客文章](https://www.category.xyz/blogs/raptorcast-designing-a-messaging-layer)，获取关于 RaptorCast 数据传输、纠删编码及广播策略的完整技术解读。


### 介绍

> 以下关于 RaptorCast 的技术说明主要涉及参与共识的验证节点之间的区块传播。需要特别说明的是，[面向全节点的区块传播](https://docs.monad.xyz/monad-arch/consensus/raptorcast#secondary-raptorcast---full-node-block-propagation)采用不同的处理机制。

在MonadBFT共识机制中，领导者需要将区块提案发送给所有验证节点。由于区块提案数据量庞大且网络环境不可靠，如何实现高效可靠的数据传播成为高性能分布式共识面临的核心挑战。

针对此问题，我们首先分析两种基础方案的局限性：
1. 领导者直连广播：领导者直接向每个验证节点发送数据。虽然实现简单，但会导致领导者上行带宽压力过大——按单交易200字节、万笔交易计算，单次提案传输就需处理2MB数据量。
2. 多级接力传播：领导者向部分节点发送数据，接收节点继续向其他节点转发。虽可降低领导者带宽压力，但会引发传输延迟累积增长，且若中间节点存在拜占庭故障可能造成数据丢失。

RaptorCast作为专为MonadBFT设计的组播消息传输协议，通过创新架构在带宽效率、传输延迟和容错能力之间取得最优平衡。以下论述中，"消息"特指区块提案数据，"消息发起方"即指共识轮次中的领导者。


### 设计需求

1. 若权益权重占比 `2/3` 以上的超级多数节点保持非故障状态（诚实且在线），则必须确保消息能可靠送达所有参与共识的节点。
2. 各验证节点的上行带宽需求需与消息大小呈线性比例关系，且与参与验证节点的总数量无关¹。
3. 最坏情况下的消息传播时间不得超过任意两节点间最差单向延迟的两倍。换言之，消息传播至所有目标接收者的耗时必须控制在网络中最远两节点间的往返时延（RTT）范围内。
4. 消息传输需配备可配置的冗余度（由节点运营方设定）。增强冗余度可有效应对数据包丢失问题，同时降低消息延迟（接收方能更快完成数据解码）。


### RaptorCast 运作机制

#### 纠删码技术
消息由发起方进行纠删编码。该编码技术将原始消息转换为若干数据块，接收方只需获取任意足够数量的数据块子集即可完整重构原始消息。

RaptorCast 采用的编码方案是基于 [RFC 5053](https://datatracker.ietf.org/doc/html/rfc5053) 所载 Raptor 码的改进版本，并针对 Monad 的特殊需求进行了以下优化：

- 提升小体量消息的编码效率
- 降低消息编码的计算复杂度（代价是解码复杂度略有提升）

#### 消息与数据块分发模型
RaptorCast 为每个数据块构建双层广播树状传输结构：消息发起方作为根节点（第0层），单个非发起方节点位于第1层，其余所有节点均处于第2层。

虽然编码消息的每个数据块理论上可对应不同的广播树，但当前实现中对编码消息块的连续区间采用了相同的广播树结构。

下图展示了该数据块分发模型的运作架构：
（此处应附广播树结构示意图，展现根节点→一级中转节点→二级接收节点的分层传播路径）

![RaptorCast Generic](/images/docs/raptorcast_generic.png)
*RaptorCast 双层广播树通用架构示意图*

采用双层广播树架构可最小化消息传输延迟。该架构中每一层的最差延迟时间不超过网络中任意两节点间的单向传输延迟（即网络的"延迟直径"），因此RaptorCast在最坏情况下的消息送达时间可控制在网络往返时延范围内。

#### 容错机制

> RaptorCast 直接基于 UDP 协议运行，每个 UDP 数据包仅承载单个消息块。  

需注意广播树采用单向传输模式。与 TCP 不同，RaptorCast 未内置针对下游节点的数据包丢失检测与重传机制，因为此类机制会破坏延迟预期。为弥补此缺陷，RaptorCast 采用冗余传输模式，由消息发起方根据网络预期丢包率动态设定冗余系数。  

例如在以下假设条件下：  
- 网络丢包率 20%  
- 至多 33% 节点存在故障或恶意行为  

消息发起方应预期最坏情况下仅有 (1 - 0.2) × (1 - 0.33) ≈ 53.6% 的数据块能抵达目标节点。为抵消该损耗，发起方需额外发送 1/0.536 - 1 ≈ 87% 的冗余数据块。  

默认采用 1480 字节 [MTU](https://en.wikipedia.org/wiki/Maximum_transmission_unit)。扣除默克尔树深度为 6 时的 RaptorCast 协议头开销，每个数据包实际可承载 1220 字节的编码有效载荷。一个 2,000,000 字节的区块将对应 2e6/1220 ≈ 1640 个源数据块。按当前冗余系数 3 计算，最终会生成 4920 个编码块，并按权益权重比例分配给各验证节点。  

若存在 100 个验证节点，这 4920 个编码块将被划分为 99 个（排除发起方）连续块区间，领导者将为每个验证节点启动对应其专属区间的广播树。若所有验证者权益均等，每个节点将接收 4920/99 ≈ 50 个连续区间数据块。

![RaptorCast Expansion](/images/docs/raptorcast_expansion.png)
*一个 2 MB 的区块会被分割成多个数据块，经过扩展后分发至全网。*

需要注意的是，这种两阶段分发模型能够确保参与共识的节点即使与消息发起方的直接网络连接出现间歇性或完全中断，仍可接收到消息副本。

![RaptorCast Monad](/images/docs/raptorcast_monad.png)
*RaptorCast 用于将经过纠删编码的数据块从领导者节点传输至各个验证者节点。*

消息发起方（领导者）通常根据权益权重将生成的数据块分配给第一跳接收节点²。例如：

验证者1 权益权重 1  
验证者2 权益权重 2  
验证者3 权益权重 3  
验证者4 权益权重 4  

当验证者1作为领导者时，将按以下比例发送数据块：  
- 2/(2+3+4) 的数据块发送至验证者2  
- 3/(2+3+4) 的数据块发送至验证者3  
- 4/(2+3+4) 的数据块发送至验证者4  

当前领导者采用连续区间方式发送数据块，但正在开发更细粒度的分发算法。新算法将支持以权益权重为比例，通过无放回随机方式向第一跳验证者分发独立或更小集合的数据块。这种方法能提升网络利用率，所有验证者可在接收数据块时立即开始处理并启动第二跳转发。

#### 数据块传输完整性
发起方会对每个编码数据块进行签名，确保广播树中的中间节点（第一层级）在转发前可验证编码数据块的完整性。

此外，源数据块数量`K`会被编码至消息中。对于给定`K`值，接收方当前接受0至`7*K-1`范围内的编码数据块。该设计既赋予发起方指定高冗余度（最高达7倍）的充分自由，又能有效限制恶意验证者的网络垃圾信息攻击。

为分摊大量数据块的签名生成与验证成本，RaptorCast采用可变深度默克尔树对连续区间的编码消息块进行聚合，并为每个默克尔树根生成单一签名。

### RaptorCast的其他应用场景
除分块广播区块外，RaptorCast还应用于：

#### 交易转发
交易转发（例如从全节点向后续三个验证者主机传输）通过RaptorCast实施，充分利用其高速性与鲁棒性。在此场景中仅需单跳传输——接收方无需重新广播。

#### 次级RaptorCast——全节点区块传播
RaptorCast 同样用于向全节点分发区块提案。如[全节点配置](https://docs.monad.xyz/node-ops/full-node-configurations)所述，每个参与验证者会建立以自身为根节点的次级RaptorCast网络，将全节点作为接收方。全节点若被验证者**优先列入名单**，或运行于**公开模式**且被筛选算法选中，则会被加入该验证者的次级 RaptorCast 组。

![RaptorCast Full Node](/images/docs/raptorcast_full_node.png)
*每个验证者在成功重构区块提案后，可通过专属连接或次级RaptorCast网络将接收（或生成）的所有数据块分发给全节点。*


次级RaptorCast架构与前述主RaptorCast图示形成镜像对应。在次级RaptorCast中，消息发起方变更为任意验证者节点，数据块接收群体则转为由公开及优先全节点组成的集合（不再采用权益加权的验证者集合）。所有参与次级RaptorCast的全节点均接收等量数据块（不适用权益权重分配机制）。

在带宽效率方面，次级RaptorCast相较专线全节点模式具有显著优势——验证者的上行带宽需求保持恒定，而不会随专线全节点数量线性增长。与主RaptorCast类似，通过引入第二跳传输机制，数据分发的负载被更均衡地分散至组内所有参与者。

**脚注**  
¹ 此特性在参与验证者权益基本持平时成立。当权益分布（极度）不均衡时，为保障在任意三分之二权益权重节点非故障场景下仍能实现可靠消息传递，需适当偏离等量上传原则。↩  
² 当需分发的数据块数量过少时（例如向100个验证者分发12个数据块），纯权益加权分配方案可能失效。该边界案例正在积极解决中。↩
 
